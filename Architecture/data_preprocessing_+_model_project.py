# -*- coding: utf-8 -*-
"""Data preprocessing + Model project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15f7wSMtYTvSN9mbHd9lBAEWamoJ7gP7Z

##Import libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""##Mount Drive"""

from google.colab import drive
drive.mount('/content/drive')

#!ls "/content/drive/My Drive"

file_path = '/content/drive/My Drive/Data.csv'
if os.path.exists(file_path):
      print("File exists")
else:
    print("File does not exist")

"""##Load the dataset"""

# Step 1:Load the dataset

df = pd.read_csv(file_path)
#print entire dataframe
print(df)

# Verify that 'Country' exists
print("Columns in df after loading:")
print(df.columns)

print("Columns in df:")
print(df.columns)

print("First few rows of df:")
print(df.head())

# Step 2: Impute missing values for the 'Country' column in the original df
categorical_imputer = SimpleImputer(strategy='most_frequent')
df['Country'] = categorical_imputer.fit_transform(df[['Country']]).ravel()

# Step 3: Define features (x) and target (y)
x = df.drop('Purchased', axis=1)
y = df['Purchased']

# Step 4: OneHotEncode the 'Country' column
onehot_encoder = OneHotEncoder()
countries_encoded = onehot_encoder.fit_transform(x[['Country']]).toarray()
country_df = pd.DataFrame(countries_encoded, columns=onehot_encoder.get_feature_names_out(['Country']))
x = pd.concat([x, country_df], axis=1).drop(['Country'], axis=1)

# Step 5: Impute missing values for numerical columns
numerical_imputer = SimpleImputer(strategy='mean')
x[['Age', 'Salary']] = numerical_imputer.fit_transform(x[['Age', 'Salary']])

# Step 5: Scale numerical columns
scaler = StandardScaler()
x[['Age', 'Salary']] = scaler.fit_transform(x[['Age', 'Salary']])

# Step 6: Verify preprocessing
print("Columns in x after preprocessing:")
print(x.columns)

"""###Feature Scaling"""

scaler = StandardScaler()
x[['Age', 'Salary']] = scaler.fit_transform(x[['Age', 'Salary']])

"""##Step 4: Splitting the data"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

print('Preprocessing Complete!')
print('Training features: \n', x_train.head())
print('Target values: \n', y_train.head())

"""###Initialize the model"""

model = LogisticRegression()
model.fit(x_train, y_train)

"""###Make predictions"""

y_pred = model.predict(x_test)

"""###Evaluate the model"""

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

"""###Print Evaluation metrics"""

print(f'Model Accuracy: {accuracy * 100:.2f}%')
print('\nConfusion Matrix:\n', conf_matrix)
print('\nClassification Report:\n', class_report)

"""###Visualize the results"""

sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""###Save and document"""

x_train.to_csv('x_train.csv', index=False)
y_train.to_csv('y_train.csv', index=False)
